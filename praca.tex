\documentclass{pracamgr}

\usepackage{polski}
\usepackage[utf8]{inputenc}

\usepackage{verbatim}
\usepackage{lmodern}
\usepackage[section]{placeins}
\usepackage[font=small]{caption}

\usepackage{amsmath,amsfonts,amssymb,amsthm,epsfig,epstopdf,titling,url,array,bm,color}

% Dane magistranta:

\author{Marek Rogala}

\nralbumu{277570}

\title{Przetwarzanie dużych grafów za pomocą Apache Giraph}

\tytulang{Processing large graphs with Apache Giraph}

\kierunek{Informatyka}

\opiekun{dra Jacka Sroki\\
  Instytut Informatyki\\
  }

\date{Wrzesień 2014}

\dziedzina{ 
%11.0 Matematyka, Informatyka:\\ 
%11.1 Matematyka\\ 
%11.2 Statystyka\\ 
11.3 Informatyka\\ 
%11.4 Sztuczna inteligencja\\ 
%11.5 Nauki aktuarialne\\
%11.9 Inne nauki matematyczne i informatyczne
}

%Klasyfikacja tematyczna wedlug AMS (matematyka) lub ACM (informatyka)
\klasyfikacja{D. Software\\
  D.127. Blabalgorithms\\
  D.127.6. Numerical blabalysis}

% Słowa kluczowe:
\keywords{graph queries, Datalog}

% WLASNE MAKRA
% TODO: uporzadkowac to
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollary}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{exmp}{Example}[section]

\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{note}{Note}

\newcommand{\todo}[1]{\textcolor{red}{@TODO: #1}}

\newcommand{\datalogra}{Datalog$^{RA}$ }
\newcommand{\aggfun}{\textit{aggfun} }
\newcommand{\aggcol}{\textit{aggcol} }

\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist 

\newcommand{\narrow}[1]{\begin{changemargin}{2cm}{2cm} #1 \end{changemargin}}


% koniec definicji

\begin{document}
\maketitle

\begin{abstract}
\begin{comment}
W~pracy przedstawiono prototypową implementację blabalizatora
  różnicowego bazującą na teorii fetorów $\sigma$-$\rho$ profesora
  Fifaka.  Wykorzystanie teorii Fifaka daje wreszcie możliwość
  efektywnego wykonania blabalizy numerycznej.  Fakt ten stanowi
  przełom technologiczny, którego konsekwencje trudno z~góry
  przewidzieć.
\end{comment}
\end{abstract}

\tableofcontents
%\listoffigures
%\listoftables

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
In recent years, the humanity has created many graph datasets much larger than those avaiable ever before.
Those graphs became a very popular object of research. Most notable examples include \emph{the Web graph} -- a graph of internet websites and links between them,
all kinds of social networks. There are also graphs such as transportation routes, similarity of newspaper of scientific articles or citations among them.

With increasing computational power and memory space, we can expect more and more real-life graphs to became subject to computation. We can also expect the existing graphs, such as the Web or social networks, to grow in all aspects.

The graps metioned can be a source of a huge amount of useful information. Hence, there is an increasing number of practical computational problems.
Some of the analyses carried out are ranking of the graph nodes (ex. importance of a Web page, determining most influential users in given group of people), clustering (detecting communities), computing metrics for the whole graph or some parts of it (ex. connectivity measures) and connection predictions.
Usually, such analyses are built on top of standard graph algorithms, such as PageRank-like procedures, shortest paths or connected components.

In the past, we have seen many tools for efficient distributed large dataset computations, starting from Google's MapReduce \cite{mapreduce} and its widely used open source counterpart, Apache's Hadoop, as well as higher-level languages such as PigLatin and Hive. However, those are not well suited for graph computations, as they do not support iteration well. 

Recently, there is an outbreak of frameworks and languages for large graphs processing, including industrial systems such as Google's Pregel \cite{pregel}, its open-source version Apache Giraph and Spark GraphX, Giraph++ \cite{giraphpp}.

The frameworks currently avaiable allow you to implement a graph algorithm in a specified model (for example Pregel's "think like a vertex"). On the other hand, query languages, such as SQL, are a bad fit for graph data since because of limited support of iteration. With the rise of graph computational problems, we need an easier way to extract information from graphs: a query language for effectively expressing data queries typical for graphs.

The Socialite \cite{socialite} \cite{distsoc} language is one of the most interesting propositions. It is based on a classical language -- Datalog. Declatarive semantics makes it easy to distribute the computations, since no execution flow is embedded in the program code. It also gives many possibilities for optimizations and approximate evaluation. At the same time, support for recursion is crucial, since most graph algorithms have iterative nature. However, most of practical graph algorithms can not be expressed efficiently in Datalog because of the language limitiations. With a few extensions to original Datalog, most important of which is recursive aggregation, SociaLite makes it easy to write intuitive programs which can be executed very efficiently.

Unfortunately, there is no solid implementation of SociaLite avaiable. The interpreter published by the authors is undocumented and contains many bugs. It is hard to imagine the langauge being implemented in the industry in the forseeable future. The papers \cite{socialite} and \cite{distsoc} which introduced SociaLite contain certain simplifications and are not specific about some important details in definitions and proofs.

The goal of this work is to bridge the gap between the theoretical idea for SociaLite and a practical implementation and to draw a path towards its usage in the industry. We show how to translate SociaLite declarative programs into Giraph "think-like-a-vertex" programs and introduce a compiler that enables SociaLite programs to be executed on existing Giraph infrastructure. This allows users of Hadoop to write and execute SociaLite programs without any additional effort to build a dedicated server infrastructure for that.

The work consists of six chapters. In \ref{r:datalog} we recall definitions of Datalog and its evaluation methods while \ref{r:pregel} contains an introduction to the Pregel computation model. In \ref{r:socialite} we describe the extensions introduced by SociaLite and provide formal definitions and general-case proofs which the original papers lack. Chapter \ref{r:s2g} shows the translation procedure from SociaLite to Giraph programs implemented in the S2G compiler, which is described in \ref{r:implementation}. In \ref{r:summary} we sketch the possible future work and the path to industrial implementation of the language using the S2G compiler.

\chapter{Datalog}\label{r:datalog}

In this chapter we define the Datalog language.

\section{Definitions}

\todo{te definicje są roboczo przepisane z \cite{wfom}, którzy z kolei podają je za \cite{fod}.}

\subsection{Queries and instances}

Let us assume that $\bm{dom}$ is an infinite set of
data values.
\begin{itemize}
\item A \emph{database schema} $\sigma$ is a collection of relation names $R$ where every $R$ has arity $ar(R) \ge 0$.
\item We call $R(\overline{d})$ a \emph{fact} when $R$ is a relation name and $\overline{d}$ is a tuple in $\bm{dom}$.
\item We say that a fact $R(d_1 , \dots , d_k)$ is \emph{over} a database schema $\sigma$ if $R \in \sigma$ and $ar (R) = k$.
\item  A \emph{(database) instance} $I$ over $\sigma$ is a finite set of facts over $\sigma$.
\item  We denote by $adom(I)$ the set of all values that occur in facts of I. When $I = {\bf{f}}$, we simply write $adom(\bf{f})$ rather than $adom(\{\bf{f}\})$.
\item By $|I|$ we denote the
number of facts in $I$.
\end{itemize}

% A \emph{query} over a schema $\sigma$ to a schema
% $\sigma'$ is a generic mapping $Q$ from instances over $\sigma$ to instances
% over $\sigma'$. Genericity means that for every permutation $\pi$ of
% $\bf{dom}$ and every instance $I$, $Q(\pi(I)) = \pi(Q(I))$. For a set of
% facts $I$ and a schema $\sigma$, we write $I|_\sigma$ to denote the maximal
%subset of $I$ that is over $\sigma$.


\subsection{Datalog with negation}
Let $\bf{var}$ be the universe of variables, disjoint from $\bf{dom}$. An atom is of the form $R(u_1 , \dots, u_k)$ where $R$ is a relation name and each $u_i \in \bf{var}$. We call $R$ the predicate. A \emph{literal} is an atom (\emph{positive atom}) or a negated atom (\emph{negative atom})


We recall Datalog with negation \cite{fod}, abbreviated Datalog$^\neg$.
Formally, a Datalog$^\neg$ rule $\phi$ is a quadruple $(head_\phi, pos_\phi,
neg_\phi , ineq_\phi)$ where $head_\phi$ is an atom; $pos_\phi$ and $neg_\phi$ are
sets of atoms; $ineq_\phi$ is a set of inequalities $(u = v)$ with
$u, v \in var$ and the variables of $\phi$ all occur in $pos_\phi$. The
components $head_\phi$, $pos_\phi$ and $neg_\phi$ are called respectively the
\emph{head}, the \emph{positive body atoms} and the \emph{negative body atoms}.
We refer to $pos_\phi \cup neg_\phi$ as the \emph{body atoms}. Note, $neg_\phi$
contains just atoms, not negative literals. Every Datalog$^\neg$
rule $\phi$ must have a head, $pos_\phi$ must be non-empty and $neg_\phi$
may be empty. If $neg_\phi = \emptyset$ then $\phi$ is called positive.

Of course, a rule $\phi$ may be written in the conventional syntax. For instance, if $head_\phi = T(u, v)$, $pos_\phi = \{R(u, v)\}$, $
neg_\phi = \{S(v)\}$, and $ineq_\phi = {u \ne v}$, with $u, v \in var$,
then we can write $\phi$ as:

$$T (u, v) :- R(u, v), \neg S(v), u \ne v$$.

The set of variables of $\phi$ is denoted $vars(\phi)$. A rule $\phi$ is
said to be over schema $\sigma$ if for each atom $R(u_1 , \dots , u_k ) \in
{head \phi } \cup pos \phi \cup neg \phi$, the arity of $R$ in $\phi$ is $k$. A Datalog program $P$ over $\sigma$ is a set of Datalog 
eg rules over $\sigma$. We write
$sch(P)$ to denote the (minimal) database schema that $P$ is
over. We define $idb(P ) \subset sch(P )$ to be the database schema
consisting of all relations in rule-heads of $P$. We abbreviate
$edb(P ) = sch(P ) \ idb(P )$. As usual, the abbreviation "idb"
stands for "intensional database schema" and "edb" stands
for "extensional database schema". A valuation for a rule
$\phi$ in $P$ w.r.t. an instance $I$ over $edb(P)$, is a total function
$V : vars(\phi) \to dom$. The application of $V$ to an atom
$R(u_1 , \dots , u_k)$ of $\phi$, denoted $V(R(u_1 , \dots, u_k ))$, results in the
fact $R(a_1 ,\dots, a_k )$ where $a_i = V (u_i )$ for each $i \in {1, . . . , k}$.
This is naturally extended to a set of atoms, which results
in a set of facts. The valuation V is said to be satisfying for
$\phi$ on $I$ if $V (pos_\phi ) \subset I, V (neg \phi ) \cap I = \emptyset$, and $V (u) \ne V (v)$ for each $(u \ne v) \in ineq_\phi$ . If so, $\phi$ is said to derive the fact $V (head_\phi )$.


\subsection{Positive and semi-positive Datalog}

A Datalog $^\neg$ program $P$ is positive if all rules of P are positive. We say
that $P$ is semi-positive if for each rule $\phi \in P$ , the atoms
of $neg_\phi$ are over $edb(P )$. We now give the semantics of a
semi-positive Datalog $^\neg$ program $P$. First, let $T_P$ be the
immediate consequence operator that maps each instance $J$
over $sch(P )$ to the instance $J = J \cup A$ where $A$ is the set
of facts derived by all possible satisfying valuations for the
rules of $P$ on $J$. Let $I$ be an instance over $edb(P )$. Consider
the infinite sequence $I_0 , I_1 , I_2 $, etc, inductively defined as
follows: $I_0 = I$ and $I_i = T_P(I_{i-1})$ for each $i \ge 1$. The output of $P$ on input $I$, denoted $P(I)$, is defined as $\bigcup_j I_j$; this
is the \emph{minimal fixpoint} of the $T_P$ operator.


\subsection{Stratified semantics} We say that $P$ is syntactically stratifiable if there is a function $\rho : sch(P ) \to {1, . . . , |idb(P )|}$
such that for each rule $\phi \in P$ , having some head predicate
$T$ , the following conditions are satisfied: 

\begin{enumerate}
\item $\rho(R) \le \rho(T )$ for each $R(\overline{u}) \in pos_\phi \cap idb(P)$
\item $\rho(R) < \rho(T )$ for each $R(\overline{u}) \in neg_\phi \cap idb(P )$.
\end{enumerate}
 For $R \in idb(P )$, we call $\rho(R)$ the stratum number of $R$. Intuitively, $\rho$ partitions $P$ into
a sequence of semi-positive Datalog $^\neg$ programs $P_1 , . . . , P_k$
with $k \le |idb(P )|$ such that for each $i = 1, \dots , k$, the program $P_i$ contains the rules of $P$ whose head predicate has
stratum number $i$. This sequence is called a syntactic stratification of $P$ . We can now apply the stratified semantics to
$P$ : for an input $I$ over $sch(P )$, we first compute the fixpoint
$P_1 (I)$, then the fixpoint $P_2 (P_1 (I))$, etc. The output of $P$ on
input $I$, denoted $P (I)$, is defined as $P_k (P_{k-1} (\dots P_1 (I) \dots))$.
It is well known that the output of $P$ does not depend on
the chosen syntactic stratification (if more than one exists).
Not all Datalog $^\neg$ programs are syntactically stratifiable. By
$stratified Datalog$ we refer to all Datalog$^\neg$ programs which
are syntactically stratifiable.

\section{Evaluation strategies for Datalog}
bottom-up, top-down

\chapter{Pregel/Giraph}\label{r:pregel}
\todo{Description of Pregel model and info about the Giraph project, also mention other extensions like Giraph++.}

\chapter{SociaLite}\label{r:socialite}

SociaLite (\cite{socialite}, \cite{distsoc}) is a graph query language based on Datalog. While Datalog allows to express some of graph algorithms
in an elegant and succint way, many practical problems cannot be efficiently solved with Datalog programs. 

SociaLite allows a programmer to write intuitive queries using declarative semantics, which can often be executed as efficiently as highly optimized dedicated programs. The queries can be executed in a distributed environment with no need for the programmer to worry about distributing the computation or managing the communication protocols.

The language consists of a few extensions to Datalog, most significant of which is the ability to combine recursive rules with aggregation. Under some conditions, such rules can be evaluated incrementally and thus as efficiently as regular recursion in Datalog.

\cite{socialite} introduces \emph{Sequential SociaLite}, intended to be executed on one machine, consisting of two main extensions: \emph{recursive aggregate functions} and \emph{tail-nested tables}. Recursive aggregate functions are the most important new feature in Socialite -- in \ref{s:recaggr} we present a complete definition and proofs of correctness of that extension, which are missing in \cite{socialite}. Tail-nested tables are a much more straightforward extension -- an optimization of data layout in memory. They are described in \ref{s:tnt}

\cite{distsoc} extends Sequential Socialite to \emph{Distributed SociaLite}, executable on a distributed architecture. It introduces a \emph{location operator}, shows how the data and computations can be distributed. The programmer does not have to think about how to distribute the data between machines or manage the communication between them. He only specifies an abstract \emph{location} for each row in the data, and the data and computations are automatically sharded. Distributed SociaLite is covered in section \ref{s:distributed}

Additionally, thanks to the declarative sematics of Datalog and SociaLite, it is possible to provide an important optimization: the \emph{delta stepping} technique, which is an effective way of parallelizing the Dijkstra algorithm \cite{deltastep}. In SociaLite, this technique can be applied automatically to a certain class of recursive aggregate programs.

In distributed computations on large graphs, an approximate result is often enough. Usually we can observe the \emph{long tail} phenomenon in the computation, where a good approximate solution is achieved quickly, but it takes a long time to get to the optimal one. In SociaLite, by simply stopping the computaion, we can obtain an approximate solution found so far. \cite{distsoc} also shows a method which can significantly reduce memory requirements by storing the intermediate results in an approximate way using Bloom filters. Those topics are covered in section \ref{s:approxdist}

\section{Datalog with recursive aggregate functions}\label{s:recaggr}

In this section we introduce the recursive aggregate functions extension from SociaLite. Since the original SociaLite consists of several extensions to Datalog, we will call the language defined here \emph{Datalog with recursive aggregate functions}, abbreviated \datalogra.

\subsection{Motivation}
Most graph algorithms are essentially some kind of iteration or recursive computation. Simple recursion can be expressed easily in Datalog. However, in many problems the computation results are gradually refined in each iteration, until the final result is reached. Examples of such algorithms are the Dijkstra algorithm for single source shortest paths or Page Rank. Usually, it is difficult or impossible to express such algorithms in Datalog efficiently, as it would require computing much more intermediate results than it is actually needed to obtain the solution. We will explain that on an example: a simple program that computes shortest paths from a source node.

A straightforward Datalog program for computing single source shortests paths (starting from node $1$) is presented below. Due to limitations of Datalog, this program computes all possible path lengths from node $1$ to other nodes in the first place, and after that for each node the minimal distance is chosen. Not only this approach results in bad performance, but causes the program to execute infinitely if a loop in the graph is reachable from the source node.

\begin{figure}[h!]
\narrow{
  \begin{flalign*}
  & \textsc{Path} (t, d) &&  & :- & && \textsc{Edge} (1, t, d). & \\
  & \textsc{Path} (t, d) &&  & :- & && \textsc{Path} (s, d_1), \textsc{Edge} (s, t, d_2), d = d_1 + d_2. & \\
  & \textsc{MinPath} (t, \textsc{Min}(d)) &&  & :- & && \textsc{Path} (t, d). &
  \end{flalign*}
  \caption{Datalog query for computing shortest paths from node 1 to other nodes}
  \label{ex:ssspdatalog}
}
\end{figure}

\datalogra allows aggregation to be combined with recursion under some conditions. This allows us to write staightforward programs for such problems, which finish execution in finite time and often are much more efficient than Datalog programs. An example \datalogra program that computes single source shortest paths is presented below. The relation $\textsc{Path}$ is declared so that for each \textit{target} the values in \textit{dist} column are aggregated using minimum operator.

\begin{figure}[h!]
\narrow{
  $\textsc{Edge}(\text{int } \textit{src}, \text{int } \textit{sink}, \text{int } \textit{len}) $ \\
  $\textsc{Path}(\text{int } \textit{target}, \text{int } \textit{dist} \text{ aggregate } \textsc{Min}) $
  \begin{flalign*}
  & \textsc{Path} (1, 0). &&  & & &&  & \\
  & \textsc{Path} (t, d) &&  & :- & && \textsc{Path} (s, d_1), \textsc{Edge} (s, t, d_2), d = d_1 + d_2. &
  \end{flalign*}
  \caption{SociaLite query for computing shortest paths from node 1 to other nodes}
}
\end{figure}

While being very useful, recursive aggregation rules not always have an unambiguous solution. This is the case only under some conditions on the rules and the aggregation function itself.

Typically, Datalog programs semantics is defined using the least fixed point of instance inclusion. This requires that the subsequent computation interations only add tuples to the database instance, but never remove tuples from the instance. This is the reason for which program \ref{ex:ssspdatalog} is inefficient. When recursive aggregate functions are allowed, this is not the case: a tuple in the instance can be replaced with a different one because a new aggregated value appeared. Consequently, in order to define SociaLite programs sematics in terms of least fixed point, we need to use a different order on database instances.

First, we will define a meet operation and show the order that it induces. Then we will show that if the aggregation function is a meet operation and corresponding rules are monotone with respect to this induced order, then the result of the program is unambiguously defined. We will also show how it can be computed efficiently.

\subsection{Meet operation and induced ordering}
\begin{defn}
A binary operation is a \emph{meet} operation if it is idempotent, commutative and associative.
\end{defn}
\todo{Maybe remind definitions of semi-lattice and partial order?}

\subsubsection{Order induced by a meet operation}

A meet operation $\sqcap$ defines a semi-lattice: it induces a partial order $\preceq_\sqcap$ over its domain, such that the result of the operation for any two elements is the least upper bound of those elements with respect to $\preceq_\sqcap$

\begin{exmp}
$\max(a, b)$ for $a, b \in \mathbb{N}$ is a meet operation; it is:
\begin{itemize}
\item idempotent -- $\max(a, a) = a$
\item commutative -- $\max(a, b) = \max(b, a)$
\item associative -- $\max(a, \max(b, c)) = \max(\max(a, b), c)$
\end{itemize}
It induces the partial order $\le$: for any two $a, b \in \mathbb{N}$, $\max(a, b)$ is their least upper bound with respect to $\le$.


On the contrary, $+$ is not a meet operation, since it is not idempotent: $1+1 \ne 1$.
\end{exmp}

\subsection{A program in \datalogra}
A \datalogra program is a Datalog program, with additional aggregation function defined for some of the relations:
For each relation $R$, there can be one column $\aggcol_R \in {1, \dots ar_R}$ chosen for which an aggregation function $\aggfun_R$ is provided. The rest of the columns are called the \emph{qualifying columns}. Intuitively, after each step of computation, we group the tuples in the relation by the qualifying columns and aggregate the column $\aggcol_R$ using $\aggfun_R$. Value $\aggcol_R = \bf{none}$ means that $R$ is a regular relation with no aggregation.

For simplicity, we assume that if a relation has an aggregated column, then it is always the last one: $\aggcol_R = ar_R$.

Syntactically, we require that each relation is declared at the top of the program as on the example below. In declaration of a relation, aggregated column can be specified by adding keyword \textit{aggregate} and name of the aggregate function next to the column declaration.

\begin{figure}[h!]
\narrow{
  $\textsc{P}(\text{int } \textit{a}, \text{int } \textit{b} \text{ aggregate } \textsc{F}) $\\
  $\textsc{R}(\text{int } \textit{src}, \text{int } \textit{sink}, \text{int } \textit{len}) $ 
  \begin{flalign*}
  & \textsc{P} (x_1, \dots, x_{ar_P}) &&  & :- & && Q_{P,1}(x_1, \dots, x_{ar_P}) & \\
  &  &&  & \dots & && & \\
  & \textsc{P} (x_1, \dots, x_{ar_P}) &&  & :- & && Q_{P,m}(x_1, \dots, x_{ar_P}) & \\
  & \textsc{R} (x_1, \dots, x_{ar_R}) &&  & :- & && Q_{R,1}(x_1, \dots, x_{ar_R}) & \\
  &  &&  & \dots & && & \\
  & \textsc{R} (x_1, \dots, x_{ar_R}) &&  & :- & && Q_{R,m}(x_1, \dots, x_{ar_R}) & \\
  \end{flalign*}
  \caption{Structure of a program in \datalogra.}
}
\end{figure}

\subsubsection{Aggregation operation $g_R$}
An important step in the evaluation of a \datalogra program is grouping the tuples in an instance of each relation and performing the aggregation within each group. We can put that into a formal definition as function $g_R$, which takes a relation instance which may contain multiple tuples with the same set of qualifying parameters and performs the aggregation.
\begin{defn}\label{d:aggregationoperationgr}
For a relation $R$ of arity $ar_R = k$, in let us define $g: \bf{dom}^k \to \bf{dom}^k$:
$$
g_R(I) = \begin{cases}
\{(x_1, \dots, x_{k-1}, \aggfun_R(\{y: (x_1, \dots, x_{k-1}, y) \in I\}): (x_1, \dots, x_{k-1}, x_k) \in I\} & \text{if } \aggcol_R \ne \bf{none} \\
I & \text{otherwise}
\end{cases}
$$
\end{defn}

If $R$ has an aggregated column, $g_R$ groups the tuples in relation instance $I$ by qualifying parameters and performs the aggregation using $\aggfun_R$. For non-aggregated relations, $g_R$ is an identity function.

\subsubsection{Order on relation instances}
In Datalog, we can prove that there is a unique least fixed point for any program. The fundamental fact needed for this proof is that during the evaluation of a Datalog program, if the state of a relation is $I_1$ at some point and $I_2$ at some other point, we know that $I_1 \subseteq I_2$. In \datalogra this property no longer holds: a tuple in $I_1$ can be replaced with different tuple with a lower value in the aggregated column. To be able to define semantics of programs in \datalogra using least fixed point, we need to use a custom order on relation instances.

\begin{defn}
Let $R$ be a relation. Let us define comparison $\sqsubseteq_R$ on relation instances as follows:
\begin{align}
I_1 \sqsubseteq_R I_2 \iff \forall_{(q_1, ..., q_{n-1}, v) \in g_R(I_1)} \exists_{(q_1, ..., q_{n-1}, v') \in g_R(I_2)} v \preceq_{\aggfun_R} v' & \text{ if } \aggcol_R \ne \bf{none} \\
I_1 \sqsubseteq_R I_2 \iff \forall_{(q_1, ..., q_n) \in g_R(I_1)} \exists_{(q_1, ..., q_n) \in g_R(I_2)} & otherwise
\end{align}
\end{defn}

\begin{note}
If $R$ does not have an aggregated column, $g_R(I) = I$ for any $I$, so $\sqsubseteq_R$ is simply the inclusion order $\subseteq$. 
\end{note}

\begin{lem}
For any $R$, $\sqsubseteq_R$ is a partial order.
\end{lem}

\emph{Proof:}

If $R$ does not have an aggregated column, $\sqsubseteq_R$ is the same as inclusion order $\subseteq$, which is a partial order.

If $R$ does have an aggregated column, then:

\begin{itemize}
\item $\sqsubseteq_R$ is reflexive: for each $R$, we have that  $\forall_{(q_1, ..., q_{n-1}, v) \in g(R)} \exists_{(q_1, ..., q_{n-1}, v) \in g(R)} v \preceq_{\aggfun_R} v $ because $\preceq_{\aggfun_R}$ is reflexive. Hence, $R \sqsubseteq_R R$.
\item $\sqsubseteq_R$ is antisimmetric \todo{No, it is not antisimmetric, so this is not a partial order --- how to deal with that?}
\item $\sqsubseteq_R$ is transistive: if $A \sqsubseteq_R B$ and $B \sqsubseteq_R  C$, then $\forall_{(q_1, ..., q_{n-1}, a) \in g(A)} \exists_{(q_1, ..., q_{n-1}, b) \in g(B)} a \preceq_{\aggfun_R} b $ and $\forall_{(q_1, ..., q_{n-1}, b) \in g(B)} \exists_{(q_1, ..., q_{n-1}, c) \in g(C)} b \preceq_{\aggfun_R} c$.

$\preceq_{\aggfun_R}$ is transistive, so $\forall_{(q_1, ..., q_{n-1}, a) \in g(A)} \exists_{(q_1, ..., q_{n-1}, c) \in g(C)} a \preceq_{\aggfun_R} c $, which means that $A \sqsubseteq_R C$.
\end{itemize}

\todo{Because of lack of antisimmetry it is only a preorder, not a partial order ---> how to deal with that?}

\todo{dodatkowy komentarz?}

\begin{exmp}
Let $R$ be a relation with arity $3$, with the last column aggregated using meet operation $\max$.
We recall that for $ \max $, $ \preceq_{\max} $ is the usual order $ \le $.
\begin{itemize}
\item $\{(1, 2, 3)\} \sqsubseteq_R \{(1, 2, 5)\}$, because $3 \le 5$
\item $\{(1, 2, 3)\} \sqsubseteq_R \{(1, 2, 5), (1, 7, 2)\}$ , because $3 \le 5$
\item $\{(1, 2, 3), (1, 2, 8)\} \sqsubseteq_R \{(1, 2, 5)\}$ , because $g_R(\{(1, 2, 3), (1, 2, 8)\}) = \{(1,2,3)\}$ and $3 \le 5$
\item $\{(1, 2, 3), (2, 8, 1)\}$ and  $\sqsubseteq_R \{(1, 2, 5), (1, 7, 2)\}$ are not comparable
\item $\emptyset \sqsubseteq_R \{(1, 2, 3)\}$
\end{itemize}
\end{exmp}

We can easily see that for any $R$ an empty relation instance $\emptyset$ is smaller under $\sqsubseteq_R$  than any other relation instance.


\subsection{Semantics and evaluation - one relation case}\label{ss:semeval1rel}
In this section we will show that the semantics of a \datalogra program can be unambiguously defined using least fixed point, as long as it satisfies some conditions. To simplify the reasoning, we will restrict our attention to programs with only one \emph{idb} relation. In the following section we extend the definitions and theorems presented here to the general case of many \emph{idb} relations.

Let $P$ be a \datalogra program, with only one \emph{idb} relation $R$ of arity $k$ in the form of:

\begin{figure}[h!]
\narrow{
%  $\textsc{R}(x_1, \dots, x_{k-1}, x_k [\text{ aggregate } \textsc{F}]) $\\
  \begin{flalign*}
  & \textsc{R} (x_1, \dots, x_k) &&  & :- & && Q_1(x_1, \dots, x_k) & \\
  &  &&  & \dots & && & \\
  & \textsc{R} (x_1, \dots, x_k) &&  & :- & && Q_m(x_1, \dots, x_k) & \\
  \end{flalign*}
}
\end{figure}

$Q_1, \dots, Q_m$ are rule bodies with free variables $x_1, \dots, x_k$. They may contain references to any of the \emph{edb} relations, which are constant during the evaluation or to the only \emph{idb} relation, $R$. We denote evaluation of rule body $Q$ in the context of an instance $I$ of the relation $Q$ as $E_I(Q)$.

Since we consider only one relation $R$, we can simplify the notation: let $\sqsubseteq$ denote $\sqsubseteq_R$ and let $g$ denote the aggregation operation $g_R$ for relation $R$, as defined in Definition \ref{d:aggregationoperationgr}.

Let us define $f: \bf{dom}^k \to \bf{dom}^k$ as the function that evaluates the rules $Q_1, ... Q_m$ based on the given instance of the relation $R$ and and returns its input extended with the set of generated tuples:
$$ f(I) = I \cup \bigcup_{i=1..m} E_I(Q_i) $$

Let $h = g \circ f$. 


\begin{thm}
If $f$ is monotone with respect to $\sqsubseteq$, i.e. $R_1 \subseteq R_2 \rightarrow f(R_1) \subseteq f(R_2)$, and there exists $n \ge 0 $, such that $h^n(\emptyset) = h^{n+1}(\emptyset)$, then $R^* = h^n(\emptyset)$ is the least fixed point of $h$, that is:
\begin{enumerate}
\item $R^* = h(R^*)$, i.e. $R^*$ is a fixpoint
\item $R^* \sqsubseteq R$ for all $R$ such that $R = h(R)$, i.e. $R^*$ is smaller than any other fixpoint
\end{enumerate}
\todo{Extract the definition of fixpoint -- it does not have to be here, but where to put it?}
\end{thm}

\emph{Proof:} 

$g$ is monotone with respect to $\sqsubseteq$. Since we assumed that $f$ is monotone with respect to $\sqsubseteq$, $h = g \circ f$ is also monotone with respect to $\sqsubseteq$. 

We know that $\emptyset$ is smaller under $\sqsubseteq$ than any other element.

Let us suppose that $I'$ is any fixpoint of $h$.  We know that $\emptyset \sqsubseteq I'$. Applying $h$ to both sides of the inequality $n$ times, we have that $I^* = h^n(\emptyset) \sqsubseteq h^n(I') = I'$, thanks to the monotonicity of $h$ with respect to $\sqsubseteq$. Therefore, the inductive fixed point $I^*$ is the least fixed point of $h$.

In pseudocode, the evaluation algorithm is straightforward:

\begin{figure}[h!]
\narrow{
$I_0 \leftarrow \emptyset$

$i \leftarrow 0$

do

{\addtolength{\leftskip}{5mm}

$i \leftarrow i + 1$

$I_i \leftarrow h(I_{i-1})$

}

while $I_i \ne I_{i-1}$


\caption{Naive evaluation algorithm for \datalogra programs with one \textit{idb} relation.}
}
\end{figure}



\todo{open questions:
\begin{itemize}
\item How we compute recursive functions with non-meet aggregation operators? -- I can forbid that for now...
\end{itemize}
}


\begin{comment}

\subsection{Semi-naive evaluation -- one relation case}
\emph{Semi-naive evaluation} is the most basic optimization used in Datalog. In comes from the following observation: in a Datalog program, if some rule $Q$ produced a tuple $t$ based on database instance $I_i$ in the $i$-th iteration of the naive evaluation algorithm, then this rule with produce this tuple in each subsequent iteration, because $I_j \supseteq I_i$ for $j > i$. The goal of this optimization is to avoid such redundant computation. It is achieved by joining only subgoals in the body of each rule which have at least one new answer produced in the previous iteration.

In this section we give the algorithm for semi-naive evaluation in \datalogra and show that this optimization is valid.

\subsubsection{Algorithm}

\begin{figure}[h!]
\narrow{
$R_0 \leftarrow \emptyset, \Delta_0 \leftarrow \emptyset$

$i \leftarrow 0$

do

{\addtolength{\leftskip}{5mm}

$i \leftarrow i + 1$

$T_i \leftarrow \bigcup_{l=1..n} f(R_{i-1})$

$R_i \leftarrow g_k(T_i \cup R_{i-1})$

$\Delta_k^i \leftarrow R_k^i - R_k^{i-1}$

}

while not for all $k$ $\Delta_k^i = \emptyset$

\caption{Semi-Naive evaluation algorithm for \datalogra programs with one \textit{idb} relation.}
}
\end{figure}

\end{comment}


\subsection{Semantics and evaluation - multiple relations case}
In this section we extend \datalogra semantics from \ref{ss:semeval1rel} to a general case of possibly many \emph{idb} relations.

Let $P$ be a \datalogra program, with $w$ \emph{idb} relations $R_1, R_2, \dots, R_w$ of arities $k_1, k_2, \dots, k_w$ respectively.

\begin{figure}[h!]
\narrow{
%  $\textsc{R}(x_1, \dots, x_{k-1}, x_k [\text{ aggregate } \textsc{F}]) $\\
  \begin{flalign*}
  & \textsc{R$_1$} (x_1, \dots, x_{k_1}) &&  & :- & && Q_{1,1}(x_1, \dots, x_{k_1}) & \\
  &  &&  & \dots & && & \\
  & \textsc{R$_1$} (x_1, \dots, x_{k_1}) &&  & :- & && Q_{1,{m_1}}(x_1, \dots, x_{k_1}) & \\
  &  &&  & \dots & && & \\
  & \textsc{R$_w$} (x_1, \dots, x_{k_w}) &&  & :- & && Q_{w, 1}(x_1, \dots, x_{k_w}) & \\
  &  &&  & \dots & && & \\
  & \textsc{R$_w$} (x_1, \dots, x_{k_w}) &&  & :- & && Q_{w, {m_w}}(x_1, \dots, x_{k_w}) & \\
  \end{flalign*}
}
\end{figure}

For $i = 1, \dots, w$, $Q_{i,1}, \dots, Q_{i,m}$ are rule bodies with free variables $x_1, \dots, x_{k_w}$. They may contain references to the \emph{idb} relations $R_1, R_2, \dots R_w$ and the \emph{edb} relations, which are constant during the evaluation. We denote evaluation of rule body $Q$ in the context of an instance $I_1, \dots I_w$ of the relation $R_1, \dots, R_w$ as $E_(I_1, \dots, I_w)(Q)$.

For $i = 1, \dots, w$, $Q_{i,1}, \dots, Q_{i,m}$, let us define $f_i: P(\bf{dom}^{k_1}) \times \dots \times P(\bf{dom}^{k_w}) \to P(\bf{dom}^{k_i})$ as the function that evaluates the rules $Q_{i,1}, ... Q_{i, m_w}$ for relation $R_i$ based on the given instances of the relations $R_1, \dots, R_w$:
$$ f_i(I_1, \dots I_w) = I_i \cup \bigcup_{i=1..{m_w}} E_(I_1, \dots, I_w)(Q_i) $$

Let $h_i = g_{R_i} \circ f_i$. Let $h(I_1, \dots, I_w) = (h_1(I_1, \dots, I_w), \dots, h_w(I_1, \dots, I_w))$

Let $\sqsubseteq = \sqsubseteq_{R_1} \times \dots \sqsubseteq_{R_w}$.

\begin{thm}
If $h$ is monotone with respect to $\sqsubseteq$, i.e. $I \subseteq I' \rightarrow h(I) \subseteq f(I')$, and there exists $n \ge 0 $, such that $h^n(\emptyset) = h^{n+1}(\emptyset)$, then $I^* = h^n(\emptyset)$ is the least fixed point of $h$, that is:
\begin{enumerate}
\item $I^* = h(I^*)$, i.e. $I^*$ is a fixpoint
\item $I^* \sqsubseteq I$ for all $I$ such that $I = h(I)$, i.e. $I^*$ is smaller than any other fixpoint
\end{enumerate}
\todo{Extract the definition of fixpoint -- it does not have to be here, but where to put it?}
\end{thm}

\emph{Proof:} 

We know that $\emptyset$ is smaller under $\sqsubseteq$ than any other element.

Let us suppose that $I'$ is any fixpoint of $h$.  We know that $\emptyset \sqsubseteq I'$. Applying $h$ to both sides of the inequality $n$ times, we have that $I^* = h^n(\emptyset) \sqsubseteq h^n(I') = I'$, thanks to the monotonicity of $h$ with respect to $\sqsubseteq$. Therefore, the inductive fixed point $I^*$ is the least fixed point of $h$.

In pseudocode, the evaluation algorithm is straightforward:

\begin{figure}[h!]
\narrow{
$I_0 \leftarrow (\emptyset, \dots, \emptyset)$

$i \leftarrow 0$

do

{\addtolength{\leftskip}{5mm}

$i \leftarrow i + 1$

$I_i \leftarrow h(I_{i-1})$

}

while $I_i \ne I_{i-1}$

\caption{Naive evaluation algorithm for \datalogra programs with multiple \textit{idb} relations.}
}
\end{figure}


\todo{open questions:
\begin{itemize}
\item How we compute recursive functions with non-meet aggregation operators? -- I can forbid that for now...
\end{itemize}
}



\subsection{\datalogra with negation}
\todo{Basically we do the same thing as in regular Datalog -- stratification}


\section{Tail-nested tables}\label{s:tnt}
Another important extension in SociaLite are \emph{tail nested tables}, which optimize the memory layout so that it can be accessed in a faster way. While being very useful in practice, this optimization is not crucial for running such programs on distributed Giraph architecture. \todo{I don't want to have this in the compiler, but maybe describe here?}

\section{Distributed SociaLite}\label{s:distributed}

\section{Delta stepping in Distributed SociaLite}\label{s:deltastep}

\section{Approximate evaluation in Distributed SociaLite}\label{s:approxdist}

\chapter{Translating SociaLite programs to Giraph programs}\label{r:s2g}

\chapter{Implementation}\label{r:implementation}

\chapter{Summary}\label{r:summary}


\begin{thebibliography}{99}
\addcontentsline{toc}{chapter}{Bibliografia}


\bibitem{socialite} Jiwon Seo, Stephen Guo, Monica S. Lam, \textit{SociaLite: Datalog extensions for efficient social network analysis}, ICDE 2013: 278-289

\bibitem{distsoc} Jiwon Seo, Jongsoo Park, Jaeho Shin, Monica S. Lam: \textit{Distributed SociaLite: A Datalog-Based Language for Large-Scale Graph Analysis}. PVLDB 6(14): 1906-1917 (2013)

\bibitem{fod} S. Abiteboul, R. Hull, and V. Vianu: \textit{Foundations of Databases}. Addison-Wesley (1995)

\bibitem{wfom} T.J. Ameloot, B. Ketsman, F. Neven, D. Zinn: \textit{Weaker Forms of Monotonicity for Declarative Networking: a more fine-grained answer to the CALM-conjecture}, PODS (2014) ...???..

\bibitem{mapreduce} Jeffrey Dean , Sanjay Ghemawat: \textit{MapReduce: simplified data processing on large clusters}, Proceedings of the 6th conference on Symposium on Opearting Systems Design \& Implementation, 2004

\bibitem{pregel} Grzegorz Malewicz, Matthew H. Austern, Aart J.C. Bik, James C. Dehnert, Ilan Horn, Naty Leiser, Grzegorz Czajkowski: \textit{Pregel: a system for large-scale graph processing}, Proceedings of the 2010 ACM SIGMOD International Conference on Management of data, 2010

\bibitem{giraphpp} Y. Tian, A. Balmin, S. A. Corsten, S. Tatikonda, J. McPherson: \textit{From "Think Like a Vertex" to "Think Like a Graph"}, Proceedings of the VLDB Endowment, 2013

\bibitem{deltastep} U. Meyer, P. Sanders: \textit{Delta-stepping: A parallel
single source shortest path algorithm.} ESA, 1998.

\bibitem{ullman} Anand Rajaraman, Jeffrey D. Ullman: \textit{Mining of Massive Datasets}, Cambridge University Press, New York, NY, 2011


\end{thebibliography}


\end{document}

