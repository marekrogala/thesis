
\chapter{Datalog}\label{r:datalog}

In this chapter we describe the basic Datalog language and its typical extended versions.

Languages based on relational algebra and relational calculus, like SQL, are widely used and researched as query languages for relational databases. This dates back to Edgar F. Codd's relational model \cite{coddrelmodel} introduced in 1970. Unfortunately, such languages leave some simple operations that they can not handle. Examples of such problems are transitive closure of a graph or distances from a vertex to all other vertices.

Datalog is a language that enhances relational calculus with recursion, which allows for solving those problems. It appeared around 1978 and is inspired by logical programming paradigm. Recently, there is an increasing interest in Datalog research as well as its implementations in industry. Datalog is typically extended with negation and simple, non-recursive aggregation.

Let us begin with an example of a problem which can not be solved in relational calculus, but can be easily solved in Datalog.

Let us suppose that we have a database with a binary relation $\textsc{Edge}$. The database represents a graph $G$: if $\textsc{Edge}(a, b)$ means that there is an edge in $G$ between vertices $a$ and $b$. Given a selected vertex $s$, we would like to find all vertices in $G$ that are reachable from $s$.

Unfortunately, unless we have some additional assumptions about $G$, is seems difficult to answer this query using languages like SQL. It can be proven that this kind of query is not expressible in the relational calculus \cite{fod}. Intuitively, what is necessary to answer such queries is some kind of conditional iteration or recursion, which is the most important feature of Datalog.

\section{History}

Datalog is not credited to any particular researchers since it originated as an extension or restriction of various other languages, including logic programming languages. It emerged as a separate area of research around 1977. It is believed that professor David Maier is the author of the name \emph{Datalog}.

Datalog is described in detail in classical books on databases theory, such as \emph{Foundations of Databases} \cite{fod}.

The language has been proven to be useful in various fields like program analysis \cite{pointanalysis}, network systems \cite{boomanalysis, dataloganalysis}. It is also used to formally define computational problems which can be solved with different models and frameworks, allowing for comparison of those frameworks and their optimizations \cite{ullman}. 

Some of the most important fields of research concerning Datalog are optimizations in programs evaluation (magic sets \cite{magicsets}, subsumptive queries \cite{subsumptivequeries}) and extensions to the language \cite{magicsetsexist, disjunctivedatalog, datalogrelaunched}.

Recently there is also an increasing interest in applications of Datalog in industry. Two examples worth mentioning are LogicBlox and Datomic. LogicBlox \cite{logicblox} delivers a high performance database which can be queried with a Datalog variant called LogiQL.  Datomic \cite{datomic} is a distributed database with an innovative architecture featuring immutable records and temporal queries, which uses Datalog as a query language.

\section{Introduction to Datalog}

Before we formally define Datalog syntax and semantics, let us take a look at an example program in this language.

As before, let us assume that the database contains a relation $\textsc{Edge}$ representing a graph and $\textsc{Edge}(a, b)$ means that there is an edge between vertices $a$ and $b$. The following program computes relation $\textsc{Tc}$ containing a transitive closure of relation $\textsc{Edge}$.

\dprog{}{
  & \textsc{Tc} (a, b) &&  & \assign & && \textsc{Edge} (a, b). & \\
  & \textsc{Tc} (a, b) &&  & \assign & && \textsc{Tc} (a, c), \textsc{Edge}(c, b). &\\
}{Datalog query for computing transitive closure of a graph}{ex:tcdatalog}

This program contains two rules. The first one states that if there is an edge between $a$ and $b$, then also there is such edge in the transitive closure. The second rule says that if there is a connection in the transitive closure between $a$ and some $c$ and at the same time there is an edge between $c$ and $b$ in the original graph, then there also exists a connection in transitive closure between $a$ and $b$. This is where recursion is used: $\textsc{Tc}$ appears on both sides of the second rule.

For example, let $\textsc{Edge}$ contain the following tuples:

\begin{centab}{ | c | }
  \hline
  $\textsc{Edge}$ \\
  \hline
  $(1, 2)$ \\
  $(2, 3)$ \\
  $(3, 4)$ \\
  $(2, 5)$ \\
  \hline
\end{centab}

The result of the program is:

\begin{centab}{ | c | }
  \hline
  $\textsc{Tc}$ \\
  \hline
  $(1, 2)$ \\
  $(1, 3)$ \\
  $(1, 4)$ \\
  $(1, 5)$ \\
  $(2, 3)$ \\
  $(2, 4)$ \\
  $(2, 5)$ \\
  $(3, 4)$ \\
  \hline
\end{centab}

As we can see, the program defines a function from the an instance of relation $\textsc{Edge}$ into an instance of relation $\textsc{Tc}$.

In the following sections, we will define Datalog's syntax and semantics in a more formal way.

\section{Datalog syntax}
Let us formally Datalog programs and rules.

\begin{defn}
A \emph{rule} is an expression of the form:
$$ \textsc{R}(x) \assign R_1(x_1), \dots, R_n(x_n). $$
where $n \ge 1$, $R, R_1, \dots, R_n$ are names of relations and $x, x_1, \dots x_n$ are tuples of free variables or constants. Each tuple $x, x_1, \dots x_n$ must have the same arity as the corresponding relation.
\end{defn}\label{d:datalogrule}

The sign $\assign$ splits the rule into two parts: the leftmost part, i.e. $R(x)$ is called the \emph{head} of the rule, while the rightmost part, i.e. $R_1(x_1), \dots, R_n(x_n)$ is called the \emph{body} of the rule. The elements of body separated by commas are called \emph{subgoals}. Head and the subgoals are called \emph{atoms}. Each atom consists of a \emph{predicate}, i.e. the relation name and \emph{arguments}.

\begin{defn}
A rule is \emph{safe} if the each free variable appearing in its head also appears in at least one of the subgoals.
\end{defn}\label{d:datalogsaferule}

\begin{defn}
A \emph{program} in Datalog is a finite set of safe rules.
\end{defn}\label{d:datalogprog}

By $adom(P)$ we denote the set of constants appearing in the rules of $P$.

The \emph{schema} of program $P$ is the set of all relation names occurring in $P$ and is denoted by $sch(P)$.

\begin{defn}
The rules of a Datalog program $P$ divide the relations into two disjoint classes: 
\begin{itemize}
\item \emph{extensional} relations, i.e. relations that occur only in the subgoals, but never in the head of the rules in $P$
\item \emph{intensional} relations occurring in the head of at least one of the rules in $P$
\end{itemize}
\end{defn}

The set of extensional relations are called the \emph{edb} or \emph{extensional database}, whereas the set of intensional relations is called \emph{idb} or \emph{intensional database}. For a program $P$, the \emph{extensional database schema}, denoted by $edb(P)$, is the set of all extensional relation names. Similarly, the \emph{intensional database schema}, denoted by $idb(P)$, is the set of all intensional relation names. 

A Datalog program is essentially a function from database instances over $edb(P)$ into database instances over $idb(P)$.

\begin{defn}
Given a rule $ \textsc{R}(x) \assign R_1(x_1), \dots, R_n(x_n). $, if $\nu$ is a valuation of variables appearing in this rule, then we obtain an \emph{instantiation} of this rule by replacing each variable $t$ in the rule by its value $\nu(t)$:
$$ \textsc{R}(\nu(x)) \assign R_1(\nu(x_1)), \dots, R_n(\nu(x_n)). $$
\end{defn}

\begin{exmp}
As an example, let us consider the following program $P$:

\rdprog{}{
  & \textsc{Mother} (parent, child) &&  & \assign & && \textsc{Parent}(parent, child), \textsc{Woman}(parent) & \\
  & \textsc{Father} (parent, child) &&  & \assign & && \textsc{Parent}(parent, child), \textsc{Man}(parent) & \\
  & \textsc{Ancestor} (ancestor, child) &&  & \assign & && \textsc{Parent} (ancestor, child) &\\
  & \textsc{Ancestor} (ancestor, child) &&  & \assign & && \textsc{Ancestor} (ancestor, parent), \textsc{Parent} (parent, child) &\\
}{}{}

Assuming that \relat{Parent}{(p, c)} means that $p$ is $c$'s parent and \relat{Woman}{(x)} and \relat{Man}{(x)} tell whether person $x$ is a woman or a man, this program computes child's father, mother and all its ancestors that can be derived.

\emph{edb} and \emph{idb} for $P$ are the following:
\begin{align*}
edb(P) =& \{\textsc{Parent}, \textsc{Man}, \textsc{Woman}\} \\
idb(P) =& \{\textsc{Mother}, \textsc{Father}, \textsc{Ancestor}\}
\end{align*}

\relat{Parent}{}, \relat{Man}{}, \relat{Man}{} are \emph{edb} relations, because there are no rules for those relations. All of their contents must be provided as an input. On the other hand, \relat{Mother}{}, \relat{Father}{}, \relat{Ancestor}{} are \emph{idb} relations, since there are rules for computing them. Only one of them, \relat{Ancestor}{} is recursively defined.

If $Anna, Chris, Patrick$ are some values in the domain, an example of an instantiation of the last rule is:
$$\textsc{Ancestor}(Anna, Chris) \assign \textsc{Ancestor} (Anna, Patrick), \textsc{Parent} (Patrick, Chris)$$
\end{exmp}

\subsection{Differences between Datalog and Prolog}
Despite the close relation between Datalog and logic programming languages, there are some significant differences:
\begin{itemize}
\item in Prolog, one can use complex terms as arguments to predicates, for example $p(s(x), y)$, which is not permitted in Datalog, where the only allowed arguments are domain elements or variables: $p(x, y)$.
\item in Prolog, there is a cut operator which is not present in Datalog. While some versions of Datalog have the notion of negation, but it is still different than the cut operator.
\item Datalog requires the rules to be \emph{safe}, which means that every variable mentioned in a rule must be also mentioned at least once in a non-negated, non-arithmetic sense
\end{itemize}


\section{Datalog semantics}
Semantics of a Datalog program can be defined using one of three different equivalent approaches.

In the \emph{model theoretic} definition, we consider the rules of program $P$ to be logical properties of the desired solution. From all possible instances of the intensional database we choose those, which are a \emph{model} for the program, i.e. satisfy all the rules. The smallest such model is defined to be the semantics of $P$.

A second approach is \emph{proof theoretic}, in which a fact is included in the result if and only if it can be derived, or proven using the rules. There are two strategies for obtaining proofs for facts: \emph{bottom up}, in which we start from all known facts and incrementally derive all provable facts, and \emph{top down}, which starts from a fact to be proven and seeks for rules and facts that can be used to prove it.

A third approach, on which we focus in this thesis is the \emph{least fix-point} semantics, which defines the result of a program as a least fix-point of some function. In this definition, a program is evaluated by iteratively applying a function until a fix-point is reached. This is very similar to the bottom-up evaluation strategy of the proof-theoretic approach.

\subsection{Fix-point semantics}
In this section we show the fix-point semantics for Datalog programs. A central notion in this definition is the \emph{immediate consequence} operator. Intuitively, that operator adds to the database new facts that could be immediately derived using one of the rules.

Given a Datalog program $P$, let $\textbf{K}$ be a database instance over $sch(P)$.

We say that a fact $R(v)$ is an \emph{immediate consequence} for $\textbf{K}$ and $P$ if $R(v) \in \textbf{K}$ or there exists an instantiation $R(v) \assign R_1(v_1), \dots, R_n(v_n)$ of a rule in $P$ such that $R_i(v_i) \in \textbf{K}$ for each $i = 1\dots n$.

The \emph{immediate consequence operator} for a Datalog program $P$ is a function $T_P: inst(sch(P)) \to inst(sch(P))$:
$$T_P(\textbf{K}) = \{ F: F \text{ is a fact over } sch(P) \text{ and $F$ is an immediate consequence for } \textbf{K} \text{ and } P \}$$

\begin{lem}
Operator $T_P$ for any Datalog program $P$ is a monotone function.
\end{lem}
\emph{Proof.} Given any $\textbf{I}, \textbf{J} \in inst(sch(P))$ such that $\textbf{I} \subseteq \textbf{J}$, let $F$ be a fact in $T_P(\textbf{I})$.
By definition, $F$ is an immediate consequence of $\textbf{I}$, so either $F$ is in $\textbf{I}$ or it there exists an instantiation
 $F \assign F_1, \dots, F_n$ of a rule in $P$ such that $F_i \in \textbf{I}$ for each $i = 1\dots n$. 
In the first case $F \in \textbf{I} \subseteq \textbf{J}$, so $F \in \textbf{J}$. 
In the second case, each $F_i \in \textbf{I} \subseteq \textbf{J}$, so the instantiation also exists in $\textbf{J}$. 
Hence, $F$ is also an immediate consequence of $\textbf{J}$, and thus $F \in T_P(\textbf{J})$. 
Since $F$ was arbitrarily chosen, we have that $T_P(\textbf{I}) \subseteq T_P(\textbf{I})$ and $T_P$ is a monotone function.

\begin{thm}
For any $P$ and an instance $\textbf{K}$ over $edb(P)$, there exists a finite minimal fix-point of $T_P$ containing $\textbf{K}$.
\end{thm}\label{t:datalogfixpointsem}
\emph{Proof.}
The definition of $T_P$ implies that $\textbf{K} \subseteq T_P(\textbf{K})$.
Because of monotonicity of $T_P$, we have inductively that $T_P^i(\textbf{K}) \subseteq T_P^{i+1}(\textbf{K})$.
Hence, we have that:
$$\textbf{K} \subseteq T_P(\textbf{K}) \subseteq T_P^2(\textbf{K}) \subseteq T_P^3(\textbf{K}) \subseteq \dots$$.

$adom(P) \cup adom(\textbf{K})$ and the database schema $sch(P)$ of $P$ are all finite, so there is a finite number $n$ of database instances over $sch(P)$ using those values. Hence, the sequence $\{T_P^i(\textbf{K})\}_i$ reaches a fix-point: $T_P^n(\textbf{K}) = T_P^{n+1}(\textbf{K})$. Let us denote this fix-point by $T_P^*(\textbf{K})$.

We will now prove that this is the minimum fix-point of $T_P$ containing $\textbf{K}$. Let us suppose that $\textbf{J}$ is a fix-point of $T_P$ containing  $\textbf{K}$:  $\textbf{K} \subseteq \textbf{J}$. By applying $T_P$ $n$ times to both sides of the inequality, we have that $T_P^*(\textbf{K}) = T_P^n(\textbf{K}) \subseteq \textbf T_P^n(\textbf{J} = \textbf{J}$. Hence, $T_P^*(\textbf{K})$ is the minimum fix-point of $T_P$ containing $\textbf{K}$.


\begin{exmp}
Let us recall the example program $P$ from the previous section:

\rdprog{}{
  & \textsc{Mother} (parent, child) &&  & \assign & && \textsc{Parent}(parent, child), \textsc{Woman}(parent) & \\
  & \textsc{Father} (parent, child) &&  & \assign & && \textsc{Parent}(parent, child), \textsc{Man}(parent) & \\
  & \textsc{Ancestor} (ancestor, child) &&  & \assign & && \textsc{Parent} (ancestor, child) &\\
  & \textsc{Ancestor} (ancestor, child) &&  & \assign & && \textsc{Ancestor} (ancestor, parent), \textsc{Parent} (parent, child) &\\
}{}{}

Given the following \emph{edb} database instance \textbf{K}:

\begin{center}
\begin{tabular}{l}
\relat{Parent}{(Anna, Bill)}\\
\relat{Parent}{(Bill, Chris)}\\
\relat{Parent}{(Anna, David)}\\
\relat{Parent}{(Chris, Eva)}\\
\end{tabular}
\quad
\begin{tabular}{l}
\relat{Woman}{(Anna)}\\
\relat{Woman}{(Eva)}\\
\relat{Man}{(Bill)}\\
\relat{Man}{(Chris)}\\
\relat{Man}{(David)}\\
\end{tabular}
\end{center}

The minimal fix-point of $T_P$ containing \textbf{K} is:

\begin{center}
\begin{tabular}{l}
\relat{Parent}{(Anna, Bill)}\\
\relat{Parent}{(Bill, Chris)}\\
\relat{Parent}{(Anna, David)}\\
\relat{Parent}{(Chris, Eva)}\\
\relat{Woman}{(Anna)}\\
\relat{Woman}{(Eva)}\\
\end{tabular}
\quad
\begin{tabular}{l}
\relat{Man}{(Bill)}\\
\relat{Man}{(Chris)}\\
\relat{Man}{(David)}\\
\relat{Mother}{(Anna, Bill)}\\
\relat{Mother}{(Anna, David)}\\
\relat{Father}{(Bill, Chris)}\\
\relat{Father}{(Chris, Eva)}\\
\end{tabular}
\quad
\begin{tabular}{l}
\relat{Ancestor}{(Anna, Bill)}\\
\relat{Ancestor}{(Bill, Chris)}\\
\relat{Ancestor}{(Anna, David)}\\
\relat{Ancestor}{(Chris, Eva)}\\
\relat{Ancestor}{(Anna, Chris)}\\
\relat{Ancestor}{(Anna, Eva)}\\
\end{tabular}
\end{center}

\end{exmp}\label{ex:ancestors}


\section{Evaluation of Datalog programs}
The most straightforward evaluation algorithm for Datalog programs is the iterative evaluation derived from the fix-point definition of semantics. While being having very simple formulation, this method is not efficient in a typical case due to excessive redundant computation. The most basic optimization adressing this problem is \emph{semi-naive} evaluation, which tries to avoid computations that can not bring any new facts. Naive and semi-naive evaluations are examples of the bottom-up strategy, where new facts are inferred based on the facts currently known.

There are also other, more optimized evaluation methods, such as Magic Sets and Subsumptive queries as well. A top-down strategy is also possible, where queries are answered by making an attempt to prove a fact using available rules.

This section briefly describes the ways to evaluate Datalog programs.

\subsection{Naive evaluation}
In naive evaluation, the computation starts with the initial database containing the \emph{edb} relations and repeatedly applies all the rules, until a fixpoint is reached.

In pseudocode, if $T_P$ is the immediate consequence operator, the algorithm for evaluation of a program $P$ on an input $\textbf{K}$ can be written as:

\parbox{0.5\textwidth}{
$P(\textbf{K}) = \{$

{\addtolength{\leftskip}{5mm}

$I_0 \leftarrow K$

$i \leftarrow 0$

\textbf{do}

{\addtolength{\leftskip}{5mm}

$i \leftarrow i + 1$

$I_i \leftarrow T_P(I_{i-1})$

}

\textbf{while} $I_i \ne I_{i-1}$

\textbf{return} $I_i$

}

$\}$
}

\begin{exmp}
As an example, let us consider the following program, which computes a transitive closure of a binary relation \relat{R}{}:
\begin{align}
\textsc{Tc}(x, y) &\assign \textsc{R}(x, y).\\
\textsc{Tc}(x, y) &\assign \textsc{Tc}(x, z), \textsc{Tc}(z, y).
\end{align}

Given $K = \{\textsc{R}(1, 2), \textsc{R}(2, 3), \textsc{R}(3, 4), \textsc{R}(4, 5)\}$, the values produced in subsequent iterations are:

\begin{align*}
I_1 \leftarrow \{&\textsc{R}(1, 2), \textsc{R}(2, 3), \textsc{R}(3, 4), \textsc{R}(2, 5), \textsc{Tc}(1, 2), \textsc{Tc}(2, 3), \textsc{Tc}(3, 4), \textsc{Tc}(4, 5)\}\\
I_2 \leftarrow \{&\textsc{R}(1, 2), \textsc{R}(2, 3), \textsc{R}(3, 4), \textsc{R}(2, 5), \textsc{Tc}(1, 2), \textsc{Tc}(2, 3), \textsc{Tc}(3, 4), \textsc{Tc}(4, 5), \\
&\textsc{Tc}(1, 3), \textsc{Tc}(2, 4), \textsc{Tc}(3, 5)\}\\
I_3 \leftarrow \{&\textsc{R}(1, 2), \textsc{R}(2, 3), \textsc{R}(3, 4), \textsc{R}(2, 5), \textsc{Tc}(1, 2), \textsc{Tc}(2, 3), \textsc{Tc}(3, 4), \textsc{Tc}(4, 5),\\
& \textsc{Tc}(1, 3), \textsc{Tc}(2, 4), \textsc{Tc}(3, 5), \textsc{Tc}(1, 4), \textsc{Tc}(2, 5)\}, \textsc{Tc}(1, 5)\}\\
\end{align*}
\end{exmp}\label{ex:naiveeval}

\subsection{Datalog is inflationary}

A simple observation is that the immediate consequence operator $T_P$ for any program $P$ is \emph{inflationary}, i. e. it possibly adds facts to the database, but can never remove any fact. In other words, $T_P(\bf{I}) \supseteq \bf{I}$ for any $I$. As a consequence, in an iterative evaluation which uses $T_P$, the database instance $\bf{I}_i$ inferred in step $i$ is a superset any of the database instance $\bf{I}_j$ that was derived in a previous step $j < i$. To name this property, we say that such semantics is \emph{inflationary}.

\subsection{Semi-Naive evaluation}
A straightforward implementation of $T_P$ definition is to perform a natural join on subgoal relations and a projection to head variables. Example \ref{ex:naiveeval} shows that such implementation may be inefficient, because most of the facts is computed more than once.

\emph{Semi-naive evaluation} is the most basic optimization used in Datalog evaluation, in which $T_P$ in an optimized way. It comes from the following observation: in a Datalog program, if some rule $Q$ produced a fact $R(t)$ based on database instance $I_i$ in the $i$-th iteration of the naive evaluation algorithm, then this rule with produce this fact in each subsequent iteration, because of the inflationary semantics of the language. The goal of this optimization is to avoid those computations after producing the fact for the first time. This is achieved by joining only subgoals in the body of each rule which have at least one new answer produced in the previous iteration. 

Let $T^\Delta_P$ denote a function that evaluates rules of program $P$ so that at least one new fact is used in application of a rule. This function needs to know which facts are the new ones, so it takes two arguments: $I$, the full database instance and $\Delta$, a database instance containing the facts that were added in the last iteration. Note that this function does not necessarily return facts from $I$, so we will need to add them to the facts newly computed to get the same result as $T_P$: $T_P(I_i) = I_i \cup T_P^\Delta(I_i, \Delta_i)$ for each $i$. The following code presents the algorithm for semi-naive evaluation: 


\parbox{0.5\textwidth}{
$P(\textbf{K}) = \{$

{\addtolength{\leftskip}{5mm}

$I_0 \leftarrow K$
$\Delta_0 \leftarrow K$

$i \leftarrow 0$

\textbf{do}

{\addtolength{\leftskip}{5mm}

$i \leftarrow i + 1$

$C_i \leftarrow T_P^\Delta(I_{i-1}, \Delta_{i-1})$

$I_i \leftarrow C_i \cup I_{i-1}$

$\Delta_i \leftarrow I_i - I_{i-1}$

}

\textbf{while} $\Delta_i \ne \emptyset$

\textbf{return} $I_i$

}

$\}$
}



\begin{exmp}
Let us consider the program and input from Example \ref{ex:naiveeval}. The facts computed by the Semi-naive evaluation in subsequent iterations would be:

$$C_1 \leftarrow \{\textsc{Tc}(1, 2), \textsc{Tc}(2, 3), \textsc{Tc}(3, 4), \textsc{Tc}(4, 5)\}$$
$$C_2 \leftarrow \{\textsc{Tc}(1, 3), \textsc{Tc}(2, 4), \textsc{Tc}(3, 5)\}$$
$$C_3 \leftarrow \{\textsc{Tc}(1, 4), \textsc{Tc}(2, 5), \textsc{Tc}(1, 5)\}$$
$$C_3 \leftarrow \{\textsc{Tc}(1, 5)\}$$
\end{exmp}\label{ex:semieval}

Semi-naive evaluation does assure that each fact will be computed once, e.g. \relat{Tc}(1, 5) was computed more than once, but it eliminates a significant portion of redundant computation.

\subsection{Other strategies}
Naive evaluation and semi-naive evaluation are examples of the bottom-up approach, where we start with the initial database instance and gradually extend it with facts that can be inferred until a fix-point is reached.

An opposite approach is possible as well. In top-down evaluation which originates in logic programs evaluation, we start with the query. For example, we would like to find all values of $x$, for which \relat{Tc}{(3, x)} is true. We can use the first rule: for \relat{Tc}{(3, x)} we would need \relat{R}{(3, x)}. The only such fact is \relat{R}{(3, 4)} for $x=4$. We can also use the second rule, which leaves us with finding $y$ such that \relat{Tc}{(3, y)}, which yields $y \in \{4\}$ by the first rule. Then we need find $x$ such that \relat{Tc}{(4, x)}, which by the first rule yields $x \in \{5\}$. The final result is thus $x \in \{4\}$

An advantage of the top-down approach is that id does not have to compute the whole database. Instead, it computes only the facts actually necessary.

This can be also achieved in bottom-up evaluation by using optimization techniques such as \emph{Magic sets} \cite{magicsets, fod} and \emph{Subsumptive queries} \cite{subsumptivequeries}. They involve transforming the relations and rules into a new program, which evaluation using the bottom-up approach essentially simulates evaluation using a top-down algorithm. Magic sets is a classical technique, while subsumptive queries is an example of a new development in the field, published in 2011.

\section{Typical extensions}
Despite recursion, pure Datalog's expressive power is still not enough for many practical applications. Datalog is often extended with:
\begin{itemize}
\item arithmetic predicates, such as $\le$
\item arithmetic functions, like addition and multiplication
\item negation
\item non-recursive aggregation
\end{itemize}

It this section we will briefly describe these extensions.

\subsection{Arithmetic predicates}
If we assume that all values in a selected column of a relation are numeric, it may be often useful to write Datalog programs that incorporate arithmetic comparisons between such values.

Let us consider a following example. We have a database of employees consisting of two relations \relat{Boss}{} and \relat{Salary}: \relat{Boss}{(a, b)} means that employee $a$ is a direct boss of employee $b$ and \relat{Salary}(a, s) means that salary of employee $a$ is $s$. We assume that all values in the second column of relation \relat{Salary}{} are numeric. We would like to find all employees that earn more than their direct boss.

\begin{center}
\begin{tabular}{|c|}
\hline
\relat{Boss}{} \\
\hline
(a, b)\\
(b, c)\\
(b, d)\\
\hline
\end{tabular}
\quad
\begin{tabular}{|c|}
\hline
\relat{Salary}{} \\
\hline
(a, 10)\\
(b, 15)\\
(c, 5)\\
(d, 20)\\
\hline
\end{tabular}
\end{center}

The following query with arithmetic comparisons solves this problem:
\begin{multline*}
\textsc{EarnsMoreThanBoss} (employee) \assign \\ \textsc{Boss}(boss, employee), \textsc{Salary}(boss, bs), \textsc{Salary}(employee, es), es > bs. \\
\end{multline*}

We can think of arithmetic comparisons as a new kind of predicates, which are infinite built-in relations. Since we introduced implicit infinite relations, we need to adjust the definition of rule safety \ref{d:datalogsaferule}:

\begin{defn}
A rule with arithmetic comparisons is \emph{safe} if each free variable appearing in its head or in any of the comparisons also appears in at least one of the non-comparison subgoals.
\end{defn}\label{d:datalogcomparisonsaferule}

This version of the requirement assures that comparisons do not introduce any new values into the database.

\subsection{Arithmetic functions}

Addition of arithmetic functions is a next step after arithmetic comparisons. In this extension, there is a new kind of subgoal, an \emph{assignment subgoal}, in the form of:
$$x = y \diamond z$$
where $x, y, z$ are free variables or constants and $\diamond$ is a binary arithmetic operation like addition, subtraction, multiplication, division etc.


An adjusted version of definition of rule safety \ref{d:datalogcomparisonsaferule} is:

\begin{defn}
A rule in Datalog with arithmetic comparisons and assignments is \emph{safe} if each free variable appearing in:
\begin{itemize}
\item its head,
\item any of the comparisons 
\item or on the right side of any of the assignment subgoals
\end{itemize}
also appears in at least one of the relational subgoals or on the left side of an assignment subgoal.
\end{defn}\label{d:datalogeqsaferule}

\begin{exmp}

As an example, let us suppose we have a graph $G$ defined by a relation $\textsc{Edge}$ 
where \relat{Edge}{(v, u, l)} means that $G$ has an edge from $v$ to $u$ of length $l > 0$. 
There is also a distinguished source vertex $s$.
An interesting question is what are the minimal distances from $s$ to all other vertices of $G$.
We will come back to this question in section \ref{ss:datalognra}.
For now, let us answer a simpler question: supposing that $G$ is a directed acyclic graph, for each vertex $v$ in $G$,
what are the lengths of paths between $s$ and $v$?

The following program answers this question using a straightforward rule of edge relaxation:

\dprog{}{
  & \textsc{Path} (v, d) &&  & \assign & && \textsc{Edge}(s, v, d) & \\
  & \textsc{Path} (v, d) &&  & \assign & && \textsc{Path} (t, d'), \textsc{Edge}(t, v, l), d = d' + l. &\\
}{Datalog query for computing all path lengths from a given source}{ex:pathsdatalog}

As we can see, arithmetic addition is crucial in this program -- it would not be possible to determine the possible path lengths without being able to generate new distance values. We can see that both rules satisfy the updated safety definition.
\end{exmp}

Introduction of arithmetic functions significantly changes the semantics.
Similarly to arithmetic comparisons, arithmetic functions can be interpreted an built-in infinite relations. 
The difference is that we do not forbid those relations to introduce new values into the database.
Given a program $P$ and a database instance $\textbf{K}$ over $sch(P)$, rules with arithmetic functions can produce new values, i.e. values that were not present in $adom(P) \cup adom(\textbf{K})$. In our example, such situation happens if there is a cycle in $G$ reachable from the source. There is an infinite number of paths from the source to the vertices on the cycle, and thus $\textsc{Path}$ would be infinite.

There are different approaches to address this problem, including \emph{finiteness dependencies} and syntactic requirements that imply safety of Datalog programs with arithmetic conditions \cite{RBS87, KRS88a, KRS88b, SV89}.

For the purpose of this paper, we can simply define semantics for Datalog programs that have a finite fixed point. The updated version of Theorem \ref{t:datalogfixpointsem} is as follows.

\begin{thm}
For any $P$ and an instance $\textbf{K}$ over $edb(P)$, if there exists $n \ge 0$ such that $T_P^n(\textbf{K})$ is a fix-point of $T_P$, then is it the minimal fix-point of $T_P$ containing $\textbf{K}$.
\end{thm}
\emph{Proof.} See second part of the proof for Theorem \ref{t:datalogfixpointsem}.

\subsection{Datalog with negation}
Pure version of Datalog permits recursion, but provides no negation. Negation allows to answer queries such as "which pairs of the nodes in graph are not connected?". There are several ways of adding negation to Datalog. One of the most prominent of them is the \emph{stratified semantics}, which we will present in this section. 

In Datalog with negation, or \datalogneg, each relational subgoal may be negated, i. e. preceded with the negation symbol $!$. The negated subgoals are called \emph{negative} subgoals, and the rest of the subgoals is called \emph{positive} subgoals. Arithmetic comparisons and assignments are not allowed to be negated.

\begin{exmp}
Let us consider the program for computing trasitive closure \relat{Tc}{} of a relation \relat{R}{} from example \ref{ex:naiveeval}. A following rule computes the pairs of nodes which are indirectly connected, i.e. are in \relat{Tc}{}, but not in \relat{R}{}:

$$\textsc{Indirect}(x, y) \assign \textsc{R}(x, y),~!\textsc{Tc}(x, y).$$
\end{exmp}

When negative subgoals are permitted, we need to include them in the definition of rules safety. 

\begin{defn}
A rule in \datalogneg with arithmetic comparisons and arithmetic assignments is \emph{safe} if each free variable appearing in:
\begin{itemize}
\item its head,
\item any of the comparisons,
\item on the right side of any of the assignment subgoals
\item or in any of its negated subgoals
\end{itemize}
also appears in at least one of the non-negated relational subgoals or on the left side of an assignment subgoal.
\end{defn}\label{d:datalogeqsaferule}

We will first consider a certain class of \datalogneg programs, called semi-positive programs, for which semantics of negation is straightforward. We will then move on to a more general version.

\begin{defn}
A \datalogneg program $P$ is \emph{semi-positive} if for each rule in $P$, all its negated subgoals are over $edb(P)$.
\end{defn}

For a semi-positive program, any relation used in a negative sense is an \emph{edb} relation, so it is constant during the evaluation of $P$. Negation could be eliminated from $P$ by introducing artificial negated \emph{edb} relations. Thus, semi-positive programs can be evaluated using the fix-point semantics just like positive Datalog programs.

The situation is different when \emph{idb} relations are used in negative subgoals. Let us assume that we use the naive evaluation. In classical Datalog, all tuples added to the database during the evaluation remain there until its end. However, when negation is allowed, it is not true in general. Let us consider a program which has a rule with a negated subgoal $!R(u)$. Such rule might produce a tuple $t$ in iteration $i$ because some $t'$ is not in $R$ and thus $!R(t')$ is true. When $t'$ is added to relation $R$ in a subsequent iteration though, the rule can no longer produce $t$. Some versions of negation semantics in Datalog allow for removing tuples from relations during the evaluation \cite{fod}.

In stratified semantics, we do not allow tuples to be removed from relations. Consequently, the inflationary semantics of Datalog is preserved. To achieve that, we require that if there is a rule for computing relation $R_1$ that uses $R_2$ in a negated subgoal, then relation $R_2$ can be fully computed before evaluation of relation $R_1$. Intuitively, this order of computation is possible if there is no direct or indirect dependency on $R_1$ in any of the rules for $R_2$, i. e. $R_1$ and $R_2$ are not recursively dependent from each other. This is formalized this by the notion of strata.

\begin{defn}
Let $P$ be a program in \datalogneg and $n = |idb(P)|$ be then number of \emph{idb} relations in $P$. A function $\rho : sch(P) \to {1, . . . , n}$ is called \emph{stratification} of $P$ if such that for each rule $\phi$ in $P$ with head predicate $T$, the following are true: 

\begin{enumerate}
\item $\rho(R) \le \rho(T)$ for each positive relational subgoal $R(u)$ of $\phi$
\item $\rho(R) < \rho(T)$ for each negative relational subgoal $R(u)$ of $\phi$.
\end{enumerate}
\end{defn}

\begin{defn}
A program that has stratification is called \emph{stratifiable}.
\end{defn}
gm
For each relation $R \in idb(P )$, $\rho(R)$ is called its \emph{stratum number}.

$\rho$ corresponds to a partitioning of $P$ into several subprograms $P_1, P_2, \dots, P_n$. Each of those programs is called a \emph{stratum} of $P$. The $i$-th stratum consists of the rules from $P$ which have a relation with stratum number $i$ in their head. We say that those relations are \emph{defined} in $P_i$.

Stratification assures that if a relation $R$ is used in rules of stratum $P_i$, then $R$ must be defined in this stratum or one of the previous strata. Additionally, if a relation is used in stratum $P_i$ in a negated subgoal, then it must be defined in an earlier stratum. It is worth noting that this allows for recursive rules, unless the recursive subgoal is negated.

For each $P_i$, $idb(P_i)$ consists of the relations defined in this stratum, while $edb(P_i)$ may contain only relations defined in earlier strata or relations from $edb(P)$. By definition of stratification, the negative subgoals in rules of $P_i$ use only relations in $edb(P_i)$. Hence, each $P_i$ is a semi-positive program and as such, it may be evaluated using the fix-point semantics.

We require the programs in \datalogneg to be stratifiable. If $P$ can be stratified into $P_1, P_2, \dots P_n$, then the output of program $P$ on input $\textbf{I}$ is defined by applying programs $P_1, P_2, \dots P_n$ in a sequence:
$$P(\textbf{I}) = P_n(\dots, P_2(P_1(\textbf{I}))\dots)$$

A program can have multiple stratifications, but it can be shown that $P(\textbf{I})$ does not depend on which of them is chosen.

\pomysl{tutaj można by dać ilustrację grafu zależności}

\subsection{Datalog with non-recursive aggregation}\label{ss:datalognra}

Datalog with negation and arithmetics is already a useful language, but for some queries one more feature is necessary: aggregation using a certain function $f$. Aggregation works similarly to $\textsc{GroupBy}$ clause in SQL: when aggregation is applied to $i$-th column of a relation, all the facts in the relation are grouped by their values in the remaining columns and for each group the value in $i$-th column is obtained by applying the aggregation function.
Let us consider the following example of relation $\textsc{Rel}$:

\begin{centab}{ | c | }
  \hline
  $\textsc{Rel}$ \\
  \hline
  $(1, 5, 5)$ \\
  $(1, 5, 3)$ \\
  $(1, 5, 4)$ \\
  $(2, 3, 4)$ \\
  $(2, 3, 5)$ \\
  $(2, 4, 6)$ \\
  \hline
\end{centab}

If aggregation with function $\textsc{Min}$ is applied to the last column of this relation, the result is a new relation $\textsc{Aggregated-Rel}$

\begin{centab}{ | c | }
  \hline
  $\textsc{Aggregated-Rel}$ \\
  \hline
  $(1, 5, 3) = (1, 5, \min{\{5, 3, 4\}})$ \\
  $(2, 3, 4) = (2, 3, \min{\{4, 5\}})$ \\
  $(2, 4, 6) = (1, 5, \min{\{6\}})$ \\
  \hline
\end{centab}

A simple version of aggregation can be introduced in Datalog by allowing the rules for aggregated relations to use only \emph{edb} relations in subgoals. The semantics and evaluation is then straightforward: such rules can be evaluated within a single application of the $T_P$ operator, and the aggregation can be applied immediately.

This definition can be extended in a simple way using the stratification method described in the previous section. Semantics for a program is defined if it can be stratified in such a way that each aggregation rule uses in its subgoals only relations defined in preceding strata. Aggregation of a relation from the same stratum, i. e. recursive aggregation, is much more complicated and is discussed in Chapter \ref{r:socialite}.

For an example, let us recall the program from example \ref{ex:pathsdatalog}, which computes for a given graph the lengths of all existing paths from source to other vertices. An interesting question in often what is the shortest path to each vertex. This question can be answered using aggregation, by computing the minimum of distances for each vertex: 

\dprog{}{
  & \textsc{Path} (v, d) &&  & \assign & && \textsc{Edge}(s, v, d) & \\
  & \textsc{Path} (v, d) &&  & \assign & && \textsc{Path} (t, d'), \textsc{Edge}(t, v, l), d = d' + l. &\\
  & \textsc{MinPath} (t, \textsc{Min}(d)) &&  & \assign & && \textsc{Path} (t, d). &
}{Datalog query for computing all path lengths from a given source}{ex:pathsdatalogaggregate}

This syntax means that after inferring all possible facts using the rule for \relat{MinPath}, this relation should be aggregated using the minimum function. If there is aggregation used in a rule for some relation, there can be no other rules for this relation.

In this example, there are two strata: \relat{Edge}{} is an \emph{edb} relation, \relat{Path}{} belongs to the first stratum and \relat{MinPath}{} belongs to the second stratum. Hence, \relat{MinPath}{} can be computed after computation of \relat{Path}{} is finished.


